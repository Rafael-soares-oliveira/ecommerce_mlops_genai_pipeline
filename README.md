# Ecommerce MLOps & GenAI Pipeline


[![Powered by Kedro](https://img.shields.io/badge/powered_by-kedro-ffc900?logo=kedro)](https://kedro.org)
[![Python](https://img.shields.io/badge/Python-3.13%2B-blue?logo=Python)](https://www.python.org/)
[![Docker](https://img.shields.io/badge/Docker-blue?logo=Docker)](https://www.docker.com/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-Docker-green?logo=Postgresql)](https://www.postgresql.org/)
[![Ollama](https://img.shields.io/badge/Ollama-Docker-green?logo=Ollama)](https://ollama.com/)

[![Ollama](https://img.shields.io/badge/theLook_eCommerce-dataset-blue?logo=GoogleCloud)](https://console.cloud.google.com/marketplace/product/bigquery-public-data/thelook-ecommerce?project=bigquery-484420)


[![CI](https://github.com/Rafael-soares-oliveira/ecommerce_mlops_genai_pipeline/actions/workflows/ci.yml/badge.svg)](https://github.com/Rafael-soares-oliveira/ecommerce_mlops_genai_pipeline/actions/workflows/ci.yml)
![Coverage](coverage.svg)

<br>

## 1. Objetivo do Projeto

O objetivo primÃ¡rio desta arquitetura Ã© fornecer uma plataforma analÃ­tica end-to-end altamente eficiente, projetada para operar em cenÃ¡rios de baixo recurso e com baixo volume de dados, mas com mÃ¡xima performance, robustez e precisÃ£o.

O sistema orquestra a ingestÃ£o e transformaÃ§Ã£o de arquivos Parquet em tabelas relacionais de mÃ©tricas de negÃ³cio, vetoriza metadados para busca semÃ¢ntica e disponibiliza uma interface interativa via Streamlit. AtravÃ©s de um Agente RAG (Retrieval-Augmented Generation), o usuÃ¡rio pode fazer perguntas em linguagem natural, que sÃ£o convertidas em consultas SQL validadas. A comunicaÃ§Ã£o entre a interface grÃ¡fica e o motor RAG/PostgreSQL ocorre via API gRPC, retornando respostas e visualizaÃ§Ãµes dinÃ¢micas com baixa latÃªncia e sem intervenÃ§Ã£o tÃ©cnica.

<br>

## 2. Arquitetura do Sistema

O fluxo de dados abaixo descreve a topologia dos containers Docker e o ciclo de vida dos dados, divididos entre o processamento em Batch (ETL) e a InferÃªncia em Tempo Real (RAG e UI Interativa).

```mermaid
graph LR
Â  Â  %% EstilizaÃ§Ã£o
Â  Â  classDef infra fill:#2d3436,stroke:#dfe6e9,color:#fff
Â  Â  classDef kedro fill:#6c5ce7,stroke:#a29bfe,color:#fff
Â  Â  classDef db fill:#00b894,stroke:#55efc4,color:#fff
Â  Â  classDef rag fill:#e17055,stroke:#fab1a0,color:#fff
Â  Â  classDef api fill:#0984e3,stroke:#74b9ff,color:#fff
Â  Â  classDef ui fill:#d63031,stroke:#ff7675,color:#fff
Â  Â  classDef viz fill:#fdcb6e,stroke:#e17055,color:#000
Â  Â  classDef cron fill:#b2bec3,stroke:#636e72,color:#000

Â  Â  subgraph Docker_Host ["Docker Host Infrastructure (Auto-detect GPU)"]
Â  Â  Â  Â  direction LR

Â  Â  Â  Â  CRON(("â° Cron")):::cron

Â  Â  Â  Â  subgraph Kedro_Group ["Data Eng (EfÃªmero)"]
Â  Â  Â  Â  Â  Â  K_WORKER["âš™ï¸ Kedro Worker <br/> (Ibis + DuckDB)"]:::kedro
Â  Â  Â  Â  Â  Â  S_TRANS["ğŸ§  Sentence-Transformers"]:::kedro
Â  Â  Â  Â  end
Â  Â  Â  Â Â 
Â  Â  Â  Â  subgraph Kedro_Viz ["Data Lineage"]
Â  Â  Â  Â  Â  Â  K_VIZ["ğŸ” Kedro Viz"]:::viz
Â  Â  Â  Â  end

Â  Â  Â  Â  subgraph Postgres_Container ["ğŸ—„ï¸ PostgreSQL 18 (Tuned)"]
Â  Â  Â  Â  Â  Â  PG_ALL[("Schemas:<br/>raw_data<br/>metrics<br/>embeddings")]:::db
Â  Â  Â  Â  end

Â  Â  Â  Â  subgraph API_Layer ["âš¡ API Layer"]
Â  Â  Â  Â  Â  Â  FASTAPI["FastAPI (gRPC)"]:::api
Â  Â  Â  Â  end

Â  Â  Â  Â  subgraph RAG_Engine ["ğŸ§  Agente RAG (Ollama GPU)"]
Â  Â  Â  Â  Â  Â  ROUTER{{"Roteador & <br/> Text-to-SQL"}}:::rag
Â  Â  Â  Â  Â  Â  VALIDATOR{"Validador<br/> (Loop)"}:::rag
Â  Â  Â  Â  end

Â  Â  Â  Â  subgraph UI_Container ["ğŸ’» Streamlit UI"]
Â  Â  Â  Â  Â  Â  INPUT[/Pergunta do UsuÃ¡rio/]:::ui
Â  Â  Â  Â  Â  Â  DASH["ğŸ“Š Dashboard DinÃ¢mico"]:::ui
Â  Â  Â  Â  end
Â  Â  end

Â  Â  %% Fluxos ETL (Batch)
Â  Â  CRON -. "Dispara" .-> K_WORKER
Â  Â  K_WORKER -- "Upsert/Calcula" --> PG_ALL
Â  Â  K_WORKER --> S_TRANS
Â  Â  S_TRANS -- "Vetoriza (pgvector)" --> PG_ALL
Â  Â  K_WORKER -- "Metadata" --> K_VIZ

Â  Â  %% Fluxos RAG & UI (Real-time)
Â  Â  INPUT -- "RequisiÃ§Ã£o RAG" --> FASTAPI
Â  Â  FASTAPI -- "ComunicaÃ§Ã£o Interna" --> ROUTER
Â  Â  ROUTER -- "Busca Contexto/Vetores" --> PG_ALL
Â  Â  ROUTER -- "Gera SQL" --> VALIDATOR
Â  Â  VALIDATOR -- "Valida Sintaxe/Esquema" --> PG_ALL
Â  Â  VALIDATOR -. "Falhou? Refaz SQL" .-> ROUTER
Â  Â  VALIDATOR -- "Resposta Final" --> FASTAPI
Â  Â  FASTAPI -- "DF + GrÃ¡fico" --> DASH
```

### 2.1. Detalhamento dos Componentes

#### Infraestrutura e Pipeline Batch (ETL)
* â° Cron: Agendador local responsÃ¡vel por disparar o script de orquestraÃ§Ã£o (`run_job.sh`) em janelas de tempo prÃ©-definidas para ingestÃ£o incremental.
* âš™ï¸ Kedro Worker (Ibis + DuckDB): Container efÃªmero que encapsula a lÃ³gica de extraÃ§Ã£o e transformaÃ§Ã£o. Utiliza a engine do DuckDB via Ibis para processar os arquivos Parquet de forma vetorizada, mitigando o alto consumo de RAM caracterÃ­stico do Pandas.
* ğŸ§  Sentence-Transfomers: NÃ³ do pipeline responsÃ¡vel por ler o dicionÃ¡rio de dados e metadados estruturados, convertendo-os em representaÃ§Ãµes vetoriais (embeddings)

<br>

## 3. Escolhas TecnolÃ³gicas e Justificativas Arquiteturais

A stack foi selecionada sob a premissa de **"foco absoluto em eficiÃªncia e baixo volume de dados"**, infraestrutura imutÃ¡vel via Docker e otimizaÃ§Ã£o de recursos de hardware.
* **Docker com DetecÃ§Ã£o de GPU (`start.sh`)**: Script de inicializaÃ§Ã£o detecta automaticamente a presenÃ§a de uma GPU Nvidia via `nvidia-smi` e aplica um override no docker-compose (`docker-compose.gpu.yml`). Isso garante que o projeto seja portÃ¡vel entre ambientes de desenvolvimento (CPU) e produÃ§Ã£o (GPU) sem alteraÃ§Ãµes manuais de cÃ³digo.
* **PostgreSQL 18 Tunado (Timescale + pgvector)**: A imagem Docker base do PostgreSQL foi customizada no `Dockerfile` e no `init.sql`.
	* A memÃ³ria `maintenance_work_mem` foi elevada para 1 GB para garantir a construÃ§Ã£o rÃ¡pida de Ã­ndices HNSW (crucial para RAG rÃ¡pido).
	* O `jit` foi desabilitado, pois em queries vetorizadas simples ele adiciona overhead desnecessÃ¡rio.
	* O agrupamento de dados (`raw_data`, `metrics`, `embeddings`) em schema distintos no mesmo banco consolida a infraestrutura relacional, analÃ­tica e vetorial.
* **Kedro Worker EfÃªmero (`run_job.sh`)**: O `kedro-worker` nÃ£o roda continuamente. Ele Ã© um container efÃªmero disparado sob demanda que morre apÃ³s concluir o pipeline de dados, liberando memÃ³ria do host. O script tambÃ©m reinicia o `streamlit` e o `kedro-viz` para limpar caches em memÃ³ria apÃ³s a carga de novos dados.
* **FastAPI (gRPC)**: Atua como a ponte de comunicaÃ§Ã£o entre o Streamlit (front-end) e o Agente RAG/Banco de Dados. O uso de gRPC garante tipagem estrita (Protobufs) e serializaÃ§Ã£o binÃ¡ria ultrarrÃ¡pida, mitigando a latÃªncia na transferÃªncia dos DataFrames e das respostas do SLM.
* **Ollama (`OLLAMA_KEEP_ALIVE=10m`)**: Para equilibrar a latÃªncia de resposta com a eficiÃªncia de infraestrutura, o Ollama foi configurado para descarregar o SLM da VRAM da GPU apÃ³s 10 minutos de inatividade (*idle*). Essa decisÃ£o de arquitetura garante que a GPU nÃ£o fique bloqueada consumindo energia desnecessariamente enquanto o RAG nÃ£o estÃ¡ em uso, aceitando um pequeno cold-start apenas na primeira requisiÃ§Ã£o de uma nova sessÃ£o de uso do Streamlit.
* **Cache de Modelos no Docker Build (UV)**: O `Dockerfile.app` utiliza a ferramenta `uv` e a montagem de cache (`--mount=type=cache`) no Docker para baixar o modelo `Sentence-Transformer` (`all-MiniLM-L6-v2`) durante a construÃ§Ã£o da imagem. Isso evita downloads redundantes a cada inicializaÃ§Ã£o dos containers, isolando o ambiente.

<br>

## 4. Pipeline de Dados: Passo a Passo e DecisÃµes de Engenharia

O projeto Ã© dividido em dois ciclos operacionais distintos: o processamento em Batch (ETL) e a inferÃªncia em tempo real.

### Fase 1: IngestÃ£o e PreparaÃ§Ã£o Batch (ExecuÃ§Ã£o EfÃªmera)

1. **ExtraÃ§Ã£o e TransformaÃ§Ã£o Ibis/DuckDB**: O Kedro executa transformaÃ§Ãµes usando o Ibis, que delega o processamento para o DuckDB. Isso garante velocidade vetorizada na leitura dos arquivos de origem (Parquet) sem consumir RAM excessiva, substituindo as operaÃ§Ãµes custosas do Pandas.
2. **Carga no PostgreSQL via Custom Dataset (Ibis Upsert)**: O Kedro nÃ£o possui um conector robusto para realizar operaÃ§Ãµes de UPSERT nativas via Ibis no PostgreSQL. O desenvolvimento de um Custom Dataset garante que os dados em `raw_data` e as mÃ©tricas geradas sejam inseridos de forma **idempotente**. ExecuÃ§Ãµes repetidas do `run_job.sh` nÃ£o duplicarÃ£o os registros.
3. **VetorizaÃ§Ã£o (Sentence-Transformers)**: ApÃ³s a criaÃ§Ã£o das mÃ©tricas, os metadados (esquemas, descriÃ§Ãµes, dicionÃ¡rios de dados) sÃ£o passados pelo cache do modelo local (configurado no `.env` via `HF_HOME`) e salvos no schema `embeddings` via `pgvector`.

### Fase 2: Motor AnalÃ­tico RAG (Tempo Real via API e UI)

1. **Input do UsuÃ¡rio**: O usuÃ¡rio envia uma pergunta na interface do Streamlit.
2. **Camada API (FastAPI gRPC)**: O Streamlit envia a requisiÃ§Ã£o para a API. A API centraliza a conexÃ£o persistente (pool de conexÃµes) com todo o banco PostgreSQL (abrangendo os schemas `embeddings`, `raw_data`, `metrics`).
3. **Busca SemÃ¢ntica (Hybrid Search)**: A API aciona o banco para realizar uma busca nos `embeddings`, utilizando recursos avanÃ§ados configurados no `init.sql` (como `pg_trgm` para buscas hÃ­bridas ou Ã­ndices dedicados), recuperando as tabelas e colunas com maior relevÃ¢ncia para a pergunta.
4. **Prompt Roteador (Text-to-SQL)**: O esquema retornado pela busca Ã© enviado ao Ollama, que gera a query SQL.
5. **Loop de ValidaÃ§Ã£o (Self-Correction)**: PrevenÃ§Ã£o de quebra do sistema por alucinaÃ§Ã£o de IA. A API tenta executar a query (ou rodar um `EXPLAIN`) no banco. Se ocorrer um erro de sintaxe ou de mapeamento (ex: coluna inexistente), o erro do PostgreSQL Ã© capturado e enviado de volta ao Ollama para correÃ§Ã£o automÃ¡tica.
6. **Entrega e VisualizaÃ§Ã£o**: Com a query validada, a API extrai o DataFrame final do PostgreSQL, decide qual o melhor tipo de grÃ¡fico e trafega a resposta final via gRPC de volta para o Streamlit renderizar o Dashboard.

<br>

## 5. Engenharia de Dados: OrquestraÃ§Ã£o e PadrÃµes com Kedro

A camada de preparaÃ§Ã£o de dados foi arquitetada sobre o framework **Kedro**, operando de forma efÃªmera e contÃªinerizada. Para garantir que o pipeline respeite as premissas de baixo consumo de recursos e alta performance, o comportamento padrÃ£o do Kedro foi estendido atravÃ©s de Hooks customizados e Custom Dataset do Kedro-datasets.

### 5.1. Observabilidade e Monitoramento de Recursos (Hooks e Logging)

Em ambientes contÃªinerizados que compartilham hardware com modelos de IA (SLMs/GPUs), vazamentos de memÃ³ria (*memory leaks*) ou picos de processamento na etapa de ETL podem derrubar o *Docker Host*. Para mitigar isso, implementamos:
* **Logging Estruturado (`logging.yml`)**: SeparaÃ§Ã£o clara entre logs informativos e de erro, com rotatividade automÃ¡tica (`RotatingFileHandler` com backup de atÃ© 50 MB no total). Isso garante que o disco nÃ£o encha com logs antigos do container efÃªmero.
* **`ResourceMonitoringHook`**: Um hook injetado no ciclo de vida do Kedro que atua como um inspetor de recursos.
	* Utiliza a biblioteca `psutil` para capturar a memÃ³ria RAM exata (*RSS*) antes e depois da execuÃ§Ã£o de cada *Node*.
	* Mede o delta de memÃ³ria e o tempo de execuÃ§Ã£o (em segundos).
	* Dispara *flags* de alerta (`HIGH MEMORY`) no log caso um nÃ³ ultrapasse o limite seguro estipulado no `parameters.yml` (ex: 1000 MB). Isso permite identificar imediatamente transformaÃ§Ãµes nÃ£o-otimizadas.

### 5.2. OtimizaÃ§Ã£o de Banco de Dados via Ciclo de Vida `CreateIndexesHook`

A manipulaÃ§Ã£o de dados em massa (Bulk Load) em tabelas que possuem Ã­ndices complexos â€” especialmente os Ã­ndices vetoriais `HNSW` do *pgvector* â€” sofre de grave degradaÃ§Ã£o de performance.
Para resolver isso, o `CreateIndexesHook` altera o fluxo padrÃ£o de DDL (Data Definition Language):
1. `before_pipeline_run`: Conecta ao PostgreSQL e executa os scripts DDL iniciais para garantir que as tabelas do schema `raw_data` existam.
2. `after_pipeline_run`: Apenas apÃ³s toda a carga de dados ser finalizada, o hook executa a criaÃ§Ã£o dos Ã­ndices (B-Tree para mÃ©tricas e HNSW para vetores). Criar Ã­ndices sobre tabelas jÃ¡ populadas Ã© mais rÃ¡pido e eficiente do que atualizar o Ã­ndice linha a linha durante o *Insert*.

### 5.3. IngestÃ£o de Alta Performance `IbisUpsertDataset`

O gargalo de qualquer pipeline ETL moderno Ã© a etapa de escrita no banco de dados. O Kedro nativo nÃ£o oferece suporte eficiente para operaÃ§Ãµes idempotentes de `UPSERT` usando Ibis/PostgreSQL. A classe `IbisUpsertDataset` foi criada para solucionar isso combinando o padrÃ£o *Factory* com serializaÃ§Ã£o em baixo nÃ­vel.
Como funciona:
1. **Zero-Copy e Arrow**: Os dados transformados pelo DuckDB sÃ£o convertidos para `PyArrow`.
2. **Protocolo BinÃ¡rio (`pgpq`)**: Em vez de gerar milhares de instruÃ§Ãµes `INSERT INTO` (que saturam a rede e a CPU), o dataset utiliza a biblioteca `pgpq` para codificar os dados do Arrow diretamente para o formato binÃ¡rio nativo do PostgreSQL.
3. **Carga em MemÃ³ria (COPY)**: Usa a instruÃ§Ã£o `COPY FROM STDIN WITH (FORMAT BINARY)` para jogar os dados em uma tabela temporÃ¡ria de forma quase instantÃ¢nea.
4. **Merge Inteligente (Upsert)**: Compara a tabela temporÃ¡ria com a tabela final. Ele gera dinamicamente uma clÃ¡usula `ON CONFLICT DO UPDATE` que sÃ³ sobrescreve o dado se a linha original e a nova forem diferentes (`IS DISTINCT FROM`). Isso economiza operaÃ§Ãµes de escrita em disco (I/O) e nÃ£o infla o *Write-Ahead Log* (WAL) do banco Ã  toa.

### 5.4. CatÃ¡logo DinÃ¢mico e DRY `catalog.yml`

O CatÃ¡logo de Dados foi desenhado seguindo o princÃ­pio *DRY* (*Don't Repeat Yourself*).
* **PadrÃµes DinÃ¢micos (`{table}`)**: Em vez de mapear dezenas de tabelas manualmente, o catÃ¡logo usa sintaxe de fÃ¡brica. A chamada `raw_{table}` mapeia automaticamente qualquer arquivo `.parquet` na camada `01_raw` atravÃ©s da engine do DuckDB em memÃ³ria.
* **YAML Anchors**: A configuraÃ§Ã£o do banco de dados (esquema de destino, credenciais, uso da classe customizada `IbisUpsertDataset`) foi encapsulada no *anchor* `&postgres_upsert_base`. Para criar uma nova entidade no pipeline, basta referenciar a base e passar o `table_name`, tornando a manutenÃ§Ã£o do projeto limpa e escalÃ¡vel.

### 5.5. Pipeline de Processamento e Qualidade de Dados (`data_processing`)

O pipeline de extraÃ§Ã£o e transformaÃ§Ã£o (`data_processing`) atua como a barreira de qualidade e integridade do Data Warehouse. Em vez de utilizar o Pandas tradicional, que carregaria todos os dados na RAM, o pipeline utiliza **Ibis** para delegar a computaÃ§Ã£o para o DuckDB (arquivos brutos) e PostgreSQL, processando dados de forma vetorizada.
### A. ValidaÃ§Ã£o de Qualidade em Passagem Ãšnica (Single-Pass Validation)

Geralmente, validaÃ§Ãµes de Data Quality (como verificar valores nulos, preÃ§os negativos ou limites geogrÃ¡ficos) exigem mÃºltiplas varreduras nos dados ou loops custosos.
* `schema_rules.py`: Define um contrato estrito de dados contendo regras granulares (linhas) e regras estruturais (agregaÃ§Ãµes, como detecÃ§Ã£o de duplicidade).
* `_validate_ibis_table`: Ã‰ o motor de regras. Em vez de validar linha a linha, esta funÃ§Ã£o compila todas as regras do contrato em um **Ãºnico bloco de agregaÃ§Ãµes Ibis** e executa a query diretamente no banco/engine. Se qualquer regra violar a condiÃ§Ã£o (retornando valor `> 0`), o pipeline aborta com um `ValueError`, detalhando exatamente a falha no log. Isso garante que "lixo nÃ£o entre" no banco de dados (*Garbage in*, *Garbage Out*).

#### B. ProteÃ§Ã£o de Integridade Referencial DinÃ¢mica (Cross-Engine Joins)

Um dos maiores desafios de cargas em bancos relacionais Ã© o erro de restriÃ§Ã£o de chave estrangeira (Foreign Key Constraint), que faz o pipeline inteiro "quebrar" na hora do `INSERT`.
Para evitar isso, o pipeline realiza a higienizaÃ§Ã£o prÃ©via dos dados atravÃ©s de uma tÃ©cnica de **Cross-Engine Join**:
1. O pipeline converte os IDs jÃ¡ validados no PostgreSQL para `PyArrow` e os carrega como uma `ibis.memtable` (tabela virtual em memÃ³ria).
2. Utiliza *joins* nos dados de origem e nos dados das *Foreign Keys* (que estÃ£o no DuckDB lendo o Parquet):
	* **Anti-Join**: Identifica registros Ã³rfÃ£os (ex: um Produto que aponta para um Centro de DistribuiÃ§Ã£o que nÃ£o existe) e os registra nos logs como *warnings* analÃ­ticos.
	* **Semi-Join**: Filtra a base original, mantendo apenas as linhas que possuem correspondÃªncia vÃ¡lida.
Isso garante que o comando final de `UPSERT` nunca falhe por falta de referÃªncias no banco.

#### C. EstratÃ©gias de Carga Incremental

Como a arquitetura visa baixo consumo de recursos, recarregar a base inteira todos os dias Ã© inviÃ¡vel. O pipeline emprega duas estratÃ©gias distintas de ingestÃ£o:
* **Watermarking**: Para a tabela `inventory_items`, o nÃ³ consulta o banco de dados destino (`target`) para descobrir a data mÃ¡xima inserida (`max(created_at)`). Apenas registros *posteriores* a esta data sÃ£o processados.
* **Moving Window**: Para tabelas transacionais altamente mutÃ¡veis (`orders` e `order_items`), o pipeline utiliza um `lookback` em dias (configurado no *parameters*). Ele processa apenas os pedidos dos Ãºltimos `X` dias, jÃ¡ que faturamentos antigos raramente sofrem atualizaÃ§Ã£o de status (de *Processing* para *Shipped*, por exemplo).

#### D. Imutabilidade e ConsistÃªncia Funcional

A transformaÃ§Ã£o de tipagem e regras de negÃ³cio (ex: garantir que a data de entrega nÃ£o seja anterior Ã  de envio) no arquivo `transform_tables.py` obedece a um fluxo puro: entra uma `ibis.Table`, sai uma `ibis.Table`.
Para injetar as validaÃ§Ãµes sem poluir a sintaxe visual do Kedro, foi criado o utilitÃ¡rio `create_node_func` (com `functools.partial`). Ele aplica os contratos de esquema de forma transparente, garantindo que o `kedro-viz` e os logs de terminal mostrem os nomes reais das funÃ§Ãµes, mantendo a observabilidade intacta.


## Tech Stack

- **Gerenciamento**: `uv` (Astral)
- **OrquestraÃ§Ã£o**: Kedro + Kedro-Viz
- **Processamento**: Ibis + PyArrow
- **Qualidade de CÃ³digo**: Ruff (Lint), Ty (Typing), Pytest (Testes)
- **Banco de Dados**: PostgreSQL + pgvector + postGIS + TimescaleDB (via Docker), DuckDB
- **Modelos AI**:
  - *Embedding*: `all_MiniLM-L6-v2` (local)
  - *SLM*: `deepseek-r1:1.5b` (via Ollama) (local)

## Estrutura do RepositÃ³rio

``` plaintext
.
â”œâ”€â”€ conf # ConfiguraÃ§Ãµes do Kedro
â”‚   â”œâ”€â”€ base/                     # ConfiguraÃ§Ãµes padrÃ£o e compartilhadas
â”‚   â”‚   â”œâ”€â”€ catalog.yml           # Data Catalog
â”‚   â”‚   â”œâ”€â”€ globals.yml           # ParÃ¢metros compartilhados entre YML
â”‚   â”‚   â””â”€â”€ parameters.yml        # ParÃ¢metros dos pipelines
â”‚
â”‚   â”œâ”€â”€ local/                    # SobrescriÃ§Ãµes locais e credenciais (ignorado no git)
â”‚   â”œâ”€â”€ logging.yml               # ConfiguraÃ§Ã£o do Logger
â”‚   â””â”€â”€ README.md                 # DocumentaÃ§Ã£o dos arquivos de configuraÃ§Ã£o
â”‚
â”œâ”€â”€ data/                         # Armazenamento local particionado pelas camadas do Kedro
â”‚   â”œâ”€â”€ 01_raw/                   # Dados brutos e imutÃ¡veis
â”‚   â”œâ”€â”€ 02_intermediate/          # Dados limpos e tipados
â”‚   â”œâ”€â”€ 03_primary/               # Dados padronizados para o modelo de domÃ­nio
â”‚   â”œâ”€â”€ 04_feature/               # Features de machine learning
â”‚   â”œâ”€â”€ 05_model_input/           # Matrizes e tensores para treinamento
â”‚   â”œâ”€â”€ 06_models/                # Modelos serializados
â”‚   â”œâ”€â”€ 07_model_output/          # InferÃªncias e prediÃ§Ãµes
â”‚   â””â”€â”€ 08_reporting/             # Dados agregados para visualizaÃ§Ã£o
â”‚
â”œâ”€â”€ junit/                        # RelatÃ³rios de testes exportados pelo Pytest/GitHub Actions
â”œâ”€â”€ logs/                         # Arquivos de log locais
â”œâ”€â”€ notebooks/                    # Rascunhos e experimentaÃ§Ãµes (ignora no git)
â”œâ”€â”€ pyproject.toml                # ConfiguraÃ§Ãµes do projeto e ferramentas
â”œâ”€â”€ README.md                     # Este arquivo
â”œâ”€â”€ sql/                          # Scripts e queries SQL
â”‚   â”œâ”€â”€ embeddings/               # Scripts para criaÃ§Ã£o e indexaÃ§Ã£o de tabelas de vetores
â”‚   â”œâ”€â”€ init.sql                  # Script de inicializaÃ§Ã£o do banco de dados (junto com o Docker)
â”‚   â”œâ”€â”€ metrics/                  # Scripts para criaÃ§Ã£o e indexaÃ§Ã£o de tabela de mÃ©tricas
â”‚   â””â”€â”€ raw_data/                 # Scripts para criaÃ§Ã£o e indexaÃ§Ã£o de tabelas pÃ³s-tratamento
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ thelook_ecommerce_analysis/
â”‚       â”œâ”€â”€ datasets/             # ImplementaÃ§Ã£o de datasets customizados
â”‚       â”œâ”€â”€ hooks.py              # Hooks de execuÃ§Ã£o do Kedro
â”‚       â”œâ”€â”€ pipeline_registry.py  # Registro central dos pipelines disponÃ­veis
â”‚       â”œâ”€â”€ settings.py           # ConfiguraÃ§Ãµes globais de execuÃ§Ã£o do Kedro
â”‚       â”œâ”€â”€ utils/                # FunÃ§Ãµes utilitÃ¡rias
â”‚       â””â”€â”€ pipelines             # Pipelines de dados
â”‚           â””â”€â”€ data_processing   # ExtraÃ§Ã£o, transformaÃ§Ã£o e carga inicial
â”‚
â”œâ”€â”€ tests/                        # Testes unitÃ¡rios espelhando a estrutura do src/
â”‚   â”œâ”€â”€ datasets/                 # Testes dos custom datasets
â”‚   â”œâ”€â”€ kedro_settings            # Testes das configuraÃ§Ãµes do Kedro
â”‚   â”œâ”€â”€ pipelines                 # Testes das lÃ³gicas dos pipelines e nodes
â”‚   â””â”€â”€ utils                     # Teste das funÃ§Ãµes utilitÃ¡rias
â”‚
â”œâ”€â”€ docker-compose.yml            # DefiniÃ§Ã£o dos serviÃ§os
â”œâ”€â”€ Dockerfile                    # Imagem do banco de dados PostgreSQL
â”œâ”€â”€ Dockerfile.app                # Imagem da aplicaÃ§Ã£o Kedro
â”œâ”€â”€ start.sh                      # Script para iniciar serviÃ§os de infra
â”œâ”€â”€ run_job.sh                    # Instancia o container efÃªmero do Kedro para execuÃ§Ã£o
â”œâ”€â”€ down.sh                       # Encerra serviÃ§os e limpa volumes
â””â”€â”€ uv.lock                       # Lockfile de dependÃªncias gerenciado pelo UV
```

## Como Executar

### PrÃ©-requisitos

- [Docker Engine](https://docs.docker.com/engine/install) (Pode ser configurado para [Podman](https://podman.io/docs/installation))
- GPU Nvidia (mudar configuraÃ§Ã£o para outras GPUs) / CPU tambÃ©m disponÃ­vel, porÃ©m mais lento

1. **Clone o repositÃ³rio**:

```
git clone https://github.com/Rafael-soares-oliveira/ecommerce_mlops_genai_pipeline
cd ecommerce_mlops_genai_pipeline
```

2. **Configure o ambiente**: Crie uma cÃ³pia do arquivo de variÃ¡veis de ambiente.
```
cp .env.example .env
```

3. **Inicie a infraestrutura base**: Este script levanta o banco de dados e o servidor Ollama, detectando automaticamente o uso de GPU.
```
bash start.sh
```

4. **Execute o pipeline de dados (ETL)**: Dispara o container efÃªmero do Kedro para ingestÃ£o, transformaÃ§Ã£o e geraÃ§Ã£o de embeddings.
```
bash run_job.sh
```

5. **Acesse as interfaces**:
* **Streamlit (UI)**: `http://localhost:8501`
* **Kedro-Viz (Lineage)**: `http://localhost:4141`
* **pgAdmin**: `http://localhost:8080`

## Tabelas de MÃ©tricas

- **MÃ©tricas de Vendas e Receita**: Focada no desempenho financeiro.
  - **Tabelas Fonte**: `order_items`, `orders`, `products`.
  - **MÃ©tricas**:
    - **GMV (Gross Merchandise Value)**: Soma total do valor das vendas (`sale_price`).
    - **Ticket MÃ©dio (AOV)**: MÃ©dia de gasto por pedido.
    - **Taxa de Cancelamento**: % de pedidos com status `Cancelled`.
- **MÃ©tricas de Clientes (CRM e RetenÃ§Ã£o)**: Focada no comportamento e valor do usuÃ¡rio ao longo do tempo.
  - **Tabelas Fonte**: `users`, `orders`.
  - **MÃ©tricas**:
    - **LTV (Lifetime Value)**: Valor total gasto por usuÃ¡rio desde o cadastro.
    - **AnÃ¡lise de Cohort**: RetenÃ§Ã£o de usuÃ¡rio agrupados pelo mÃªs de aquisiÃ§Ã£o (safra).
    - **RFM (RecÃªncia, FrequÃªncia, MonetÃ¡rio)**: SegmentaÃ§Ã£o de clientes para marketing.
    - **Novos vs. Recorrentes**: ProporÃ§Ã£o de vendas de primeira compra vs. recompra.
- **MÃ©tricas de Produto e Estoque**: Focada na logÃ­stica e atratividade do item.
  - **Tabelas Fonte**: `inventory_items`, `products`, `order_items`, `distribution_center`.
  - **MÃ©tricas**:
    - **Taxa de DevoluÃ§Ã£o**: % de itens com status `Returned`.
    - **Tempo de Envio**: DiferenÃ§a entre `created_at` e `shipped_at`.
    - **Margem de Produto**: DiferenÃ§a entre `sale_price` e `cost`.
    - **Aging do Estoque**: Tempo que os itens ficam no inventÃ¡rio antes da venda.
- **MÃ©tricas de NavegaÃ§Ã£o (Web Analytics)**: Focada no funil de conversÃ£o no site.
  - **Tabelas Fonte**: `events`.
  - **MÃ©tricas PossÃ­veis**:
    - **Taxa de ConversÃ£o de SessÃ£o**: Visitantes Ãºnicos que compram / Total de visitantes.
    - **Abandono de Carrinho**: UsuÃ¡rios que adicionam ao carrinho (`cart`) mas nÃ£o compram (`purchase`).
    - **Origem de TrÃ¡fego**: AnÃ¡lise da coluna `traffic_source`.

## Roadmap de ImplementaÃ§Ã£o (Planejamento)

Este planejamento foca nas entregas lÃ³gicas, sem datas fixas.

### Fase 1: FundaÃ§Ã£o & Infraestrutura

- [X] Configurar repositÃ³rio com `.gitignore` e `pyproject.toml`.
- [X] Criar `docker-compose.yml`, `Dockerfile` e scripts `sh`.
- [X] Configurar/Validar conexÃ£o com o Banco de Dados.

### Fase 2: Core Engineering (Kedro ETL)

- [X] Inicializar projeto Kedro (`kedro new`).
- [X] Configurar `crendentials.yml` e `parameters.yml`.
- [X] Configurar `conf/base/logging.yml` e configurar Hooks do Kedro.
- [X] Criar testes unitÃ¡rios para testar hooks.py e settings.py
- [X] Registrar datasets no `catalog.yml`.
- [X] Implementar **Pipeline de TransformaÃ§Ã£o**:
  - [X] Limpeza com Ibis.
  - [X] LÃ³gica de Watermark (Upsert) lendo do Postgres.
- [X] Criar testes com pelo menos 90% coverage
- [X] Configurar pipeline de CI (GitHub Actions) para rodar `ruff`, `ty`, `pytest` e gerar relatÃ³rios.
- [X] Documentar no README.md

### Fase 3: MÃ©tricas, Vetores e AI

- [ ] Implementar **Pipeline de Embeddings**:
  - [ ] Node para gerar vetores de descriÃ§Ãµes de produtos.
  - [ ] Criar testes com pelo menos 90% coverage
  - [ ] Documentar no README.md
- [ ] Implementar **Pipeline de MÃ©tricas**:
  - [ ] Node para criar tabelas de mÃ©tricas
  - [ ] Criar testes com pelo menos 90% coverage
  - [ ] Documentar no README.md
- [ ] Implementar **Pipeline de SLM Batch**:
  - [ ] Configurar modelo e contexto
  - [ ] Node que agrega mÃ©tricas diÃ¡rias.
  - [ ] IntegraÃ§Ã£o com API do Ollama para gerar resumos textuais.

### Fase 4: Consumo e VisualizaÃ§Ã£o

- [ ] Configurar API gRPC e Protobufs.
- [ ] Configurar Streamlit.
- [ ] Cria dashboard modelo no Streamlit.
- [ ] Criar Dashboard no Streamlit.
- [ ] Implementar Chatbot RAG no Streamlit.
